
## Appendix: Details of the Analysis Process

1. `r step_1`

  + Variable: `paths_info`.

  + I wrote a wrapper function for `download.file()` (`download_report()`) to assist with this. This helper function specified the correct url given a year.

2. `r step_2`

  + Variable: `pages_nest`.

  + I used `purrr::map()` and `pdftools::pdf_text()` with `year` as a grouping variable.
  
  + Notably, this data frame has each report stored as a list column named `page`.

```{r pages_nest-1, include=F, echo=F, eval=F}
pages_nest
```


3. `r step_3`

  + Noteably, I added a index `idx_page` (via `dplyr::row_number()`). However, `idx_page` doesn't correspond directly to the page numbers of the reports because the reports have various type of pages leading up to the first numbered page---title page, blank pages, a table of contents, and an exective summary. The numbered pages in each report begin somewhere in the 30s---it varies for each report---in terms of the "raw" PDF page number.
  
  + This `pages` data frame has each page's raw text stored an individual row. Of course, this is quite messy. For example, the second page of the executive summary reads like `"    Executive Summary\r\n    Review of Real-Time Market Outcomes\r\n    Although only a small share [...]"`.

  + Initially, I had hoped to use an approach like that demonstrated by #rstats aficionado [`hrbrmstr`]() in [this post about parsing pdf tables](https://rud.is/b/2017/01/26/one-view-of-the-impact-of-the-new-immigration-ban-freeing-pdf-data-with-tabulizer/), but I concluded that the data was too "irregular" to warrant this approach. Thus, I turned to every data scientist's/software engineer's best friend---[regular expressions](https://en.wikipedia.org/wiki/Regular_expression)! (Please acknowledge the sarcasm :).) Thankfully, the data was uniform enough to be parsed in this manner.

4. 

  + For this task, I wrote and used a custom function `convert_page_to_lines()` to create the data frame `toc` with rows corresponding to each line of the table of contents sections.

  + These rows look like the following.

```{r toc_ex_show-1, eval=F}
toc_ex
```

  + Note that the `convert_page_to_lines()` function is key to this cleaning step. It uses an idiom that seems like it could be applied to many kinds of text parsing---namely, `stringr::str_split(x, "\\n") %>% purrr::map(stringr::str_squish) %>% unlist() %>% data frame::enframe()`.

+ + "Augment" and "clean" the `toc` data frame with columns for `label`, `index` (the , and `page_num`.
  + For example

```{r toc_aug_ex_show-1, eval=F}
toc_aug_ex
```

+ Unnest, clean, and augment `toc_pages` data frame, creating the `toc_aug` data frame.

  + Create the supplementary data frame `body_rngs` to "map" the page numbers to each report.

  + The data frame `pages_n` is created beforehand to aid with the mapping.

